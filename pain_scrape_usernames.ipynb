{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1O14wqmgtaU3RK_GJHRDySv-CySyraoAN","authorship_tag":"ABX9TyNQxjOMBkMY5HQ8k6g9tzPp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#install the library to scrape pushshift\n","!pip install pmaw pandas"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eUKgmsXpJKe-","executionInfo":{"status":"ok","timestamp":1680742520073,"user_tz":240,"elapsed":13336,"user":{"displayName":"Luis Ruiz","userId":"02721672642318796029"}},"outputId":"656d9797-d5d4-4475-cc01-cb3c8d0f9637"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pmaw\n","  Downloading pmaw-3.0.0-py3-none-any.whl (29 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.4.4)\n","Collecting praw\n","  Downloading praw-7.7.0-py3-none-any.whl (189 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.4/189.4 KB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pmaw) (2.27.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2022.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from pandas) (1.22.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","Collecting prawcore<3,>=2.1\n","  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\n","Collecting update-checker>=0.18\n","  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n","Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.9/dist-packages (from praw->pmaw) (1.5.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->pmaw) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pmaw) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->pmaw) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->pmaw) (2022.12.7)\n","Installing collected packages: update-checker, prawcore, praw, pmaw\n","Successfully installed pmaw-3.0.0 praw-7.7.0 prawcore-2.3.0 update-checker-0.18.0\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from pmaw import PushshiftAPI\n","from datetime import datetime, timedelta\n","import numpy as np\n","from itertools import product\n","import unicodedata as ud\n","\n","# Define subreddit to scrape\n","sub = 'Chronicpain'\n","\n","# Define scrape window & time delta\n","start_date = datetime(2022, 11, 3)\n","end_date = datetime(2023, 4, 3)\n","delta = 1 #weeks\n","\n","# Initialize pushshift api instance\n","api = PushshiftAPI()\n","\n","# Scrape\n","dfs = []  # to store the results of each api call\n","\n","#Scrapping sub in 1-week-long time windows\n","current_date = start_date\n","while current_date <= end_date:\n","  since = int(current_date.timestamp())  # start date\n","  until = int((current_date + timedelta(weeks=delta)).timestamp())      # end date\n","  print(current_date.strftime('%d %b %Y'))\n","  # Only scrapping 1000 comments per call because api breaks for >1000\n","  comments = api.search_comments(         \n","                      subreddit=sub,\n","                      since = since,\n","                      until = until,\n","                      limit=1000,\n","                      fields=[['author','author_fullname','subreddit','subreddit_id','id','body','author_flair_text', 'created_utc']],\n","                      safe_exit=True\n","                      )\n","  # Some days might have no user activity, so we need try/catch\n","  try:\n","    # Place results in dataframe\n","    df = pd.DataFrame(comments)\n","    # Keep only relevant info\n","    df = df[['subreddit','subreddit_id','author','author_fullname','body', 'created_utc','id']]\n","    # Save to CSV file\n","    fname = '/content/drive/MyDrive/Classes/EECS_448/Project/pain_scrape/{}_{}_{}_{}.csv'.format(sub, str(since), str(until), len(df.index))\n","    df.to_csv(fname)\n","    dfs.append(df)\n","  except:\n","    print('Exception raised. r/{}, search date: {}'.format(sub, since))\n","  # Increase the data tracker\n","  current_date += timedelta(weeks=delta)\n","\n","all_df = pd.concat(dfs, ignore_index=True)"],"metadata":{"id":"1LfAHnHgJNcT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680744626946,"user_tz":240,"elapsed":311542,"user":{"displayName":"Luis Ruiz","userId":"02721672642318796029"}},"outputId":"96a45f34-2bbe-4eb6-86b6-4cad4e75571b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["03 Nov 2022\n","10 Nov 2022\n","17 Nov 2022\n","24 Nov 2022\n","01 Dec 2022\n","08 Dec 2022\n","15 Dec 2022\n","22 Dec 2022\n","29 Dec 2022\n","05 Jan 2023\n","12 Jan 2023\n","19 Jan 2023\n","26 Jan 2023\n","02 Feb 2023\n","09 Feb 2023\n","16 Feb 2023\n","23 Feb 2023\n","02 Mar 2023\n","09 Mar 2023\n","16 Mar 2023\n","23 Mar 2023\n","30 Mar 2023\n"]}]},{"cell_type":"code","source":["# save dataframe of entire scrape to csv\n","fname = '/content/drive/MyDrive/Classes/EECS_448/Project/pain_fullScrape.csv'\n","all_df.to_csv(fname)\n","\n","# make authors dataframe\n","authors_df = all_df[['author']]\n","authors_df = authors_df.drop_duplicates()\n","\n","# save authors dataframe to csv\n","fname = '/content/drive/MyDrive/Classes/EECS_448/Project/pain_allLabledAuthors.csv'\n","authors_df.to_csv(fname)\n","authors_df.head()\n","\n","print(len(authors_df))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ow_hTrejPN3","executionInfo":{"status":"ok","timestamp":1680744758313,"user_tz":240,"elapsed":550,"user":{"displayName":"Luis Ruiz","userId":"02721672642318796029"}},"outputId":"c8f4b4bd-3c1c-47e0-b210-c5085aa10f9d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["5202\n"]}]},{"cell_type":"markdown","source":["### Playground / Graveyard"],"metadata":{"id":"c_z4MjSUcVsv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"H2yq-HMMI1j8"},"outputs":[],"source":["import pandas as pd\n","from pmaw import PushshiftAPI\n","import datetime as dt\n","import numpy as np\n","from itertools import product\n","import unicodedata as ud\n","\n","def format_df(df_original):\n","  # Keep only relevant columns\n","  df = df_original.copy()\n","  df = df[['subreddit','subreddit_id','author','author_fullname','author_flair_text','body', 'created_utc','id']]\n","  # Clean the flair text\n","  df['author_flair_text'] = df['author_flair_text'].apply(clean_flair)\n","  # Keep only rows with flair\n","  df = df[df['author_flair_text'].notna()]\n","  return df\n","\n","def clean_flair(flair):\n","  if type(flair) == str:\n","    mbti_types = [''.join(tup) for tup in list(product(['e', 'i'],['s', 'n'], ['f', 't'],['j', 'p']))]\n","    flair = ''.join(e for e in flair if e.isalpha()) #strip non-alphabetical characters\n","    flair = ud.normalize('NFKD', flair) #normalize unicode representation\n","    flair = flair.lower() #make lowercase\n","    try:\n","      # Extract the mbti type from the flair string by:\n","      # (1) Find occurences of mbti types in flair and store the index of the first occurence\n","      # example: 'anesfpmarriedtointp' returns 2, the index of the character 'e'\n","      idx = min(i for i in [flair.find(x) for x in mbti_types] if i >= 0)\n","      #(2) make the flair be the first mbti occurence, example above returns 'esfp'\n","      flair = flair[idx:idx+4]\n","    except:\n","      # No match was found, so not useful\n","      flair = np.nan\n","  else:\n","    flair = np.nan\n","  return flair\n","\n","# Make a list w all subreddit name (ie mbti types)\n","subreddits = [''.join(tup) for tup in list(product(['e', 'i'],['s', 'n'], ['f', 't'],['j', 'p']))]\n","\n","# define lists with years and months used to define scrapping time window\n","years = [2022, 2022, 2023, 2023, 2023]\n","months = [11, 12, 1, 2, 3]\n","\n","# Initialize pushshift api instance\n","api = PushshiftAPI()\n","\n","# Scrape\n","dfs = []  # to store the results of each api call\n","\n","for sub in subreddits:\n","  #Scrapping each sub in 1-month-long time windows\n","  for i in range(len(years)-1):\n","    start_year, end_year, start_month, end_month = years[i], years[i+1], months[i], months[i+1]\n","    since = int(dt.datetime(start_year, start_month, 3,0,0).timestamp())  # start date\n","    until = int(dt.datetime(end_year, end_month, 2,0,0).timestamp())      # end date\n","    # Only scrapping 1000 comments per call because api breaks for >1000\n","    comments = api.search_comments(         \n","                        subreddit=sub,\n","                        since = since,\n","                        until = until,\n","                        limit=1000,\n","                        fields=[['author','author_fullname','subreddit','subreddit_id','id','body','author_flair_text', 'created_utc']],\n","                        safe_exit=True\n","                        )\n","    # Place results in dataframe\n","    df = pd.DataFrame(comments)\n","    # Format the results\n","    df = format_df(df)\n","    # Save to CSV file\n","    fname = '/content/drive/MyDrive/Classes/EECS_448/Project/scrapping_files/{}_{}_{}_{}.csv'.format(sub, str(since), str(until), len(df.index))\n","    df.to_csv(fname)\n","    dfs.append(df)\n","\n","all_df = pd.concat(dfs, ignore_index=True)"]},{"cell_type":"code","source":["# all_df.columns\n","# test = (all_df.author_flair_css_class.to_numpy())\n","\n","# np.unique(all_df.author_fullname.to_numpy()).shape\n","# all_df.subreddit.unique()\n","# all_df.apply(lambda x: x.astype(str).str.lower()).subreddit.unique()\n","# all_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PS5gYT4xURMb","executionInfo":{"status":"ok","timestamp":1679535474241,"user_tz":240,"elapsed":3,"user":{"displayName":"Luis Ruiz","userId":"02721672642318796029"}},"outputId":"d3721870-416b-49fb-a929-9099e616a218"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5398,)"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["authors_df = all_df[['author','author_flair_text']]\n","authors_df = authors_df.drop_duplicates()\n","authors_df.index"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dX9e2MnWGlZM","executionInfo":{"status":"ok","timestamp":1679535652050,"user_tz":240,"elapsed":136,"user":{"displayName":"Luis Ruiz","userId":"02721672642318796029"}},"outputId":"2f91dd4d-cbde-4da8-ec0e-bd9b67a61a92"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Int64Index([    0,     1,     2,     3,     4,     5,     6,     9,    12,\n","               16,\n","            ...\n","            28216, 28218, 28220, 28224, 28228, 28229, 28231, 28232, 28236,\n","            28239],\n","           dtype='int64', length=5504)"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["# api = PushshiftAPI()\n","# dfs = []\n","# for sub in subreddits:\n","#   for i in range(len(years)-1):\n","#     start_year, end_year, start_month, end_month = years[i], years[i+1], months[i], months[i+1]\n","#     since = int(dt.datetime(start_year, start_month, 3,0,0).timestamp())\n","#     until = int(dt.datetime(end_year, end_month, 2,0,0).timestamp())\n","#     comments = api.search_comments(         \n","#                         subreddit=sub,\n","#                         since = since,\n","#                         until = until,\n","#                         limit=1000,\n","#                         safe_exit=True\n","#                         )\n","#     df = pd.DataFrame(comments)\n","#     fname = '/content/drive/MyDrive/Classes/EECS_448/Project/scrapping_files/{}_{}_{}_{}.csv'.format(sub, str(since), str(until), len(comments))\n","#     df.to_csv(fname)\n","#     dfs.append(df)\n"],"metadata":{"id":"GiCt8_N4Jt1r"},"execution_count":null,"outputs":[]}]}