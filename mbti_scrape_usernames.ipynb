{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1TixvmZLqP9ozPqAaeFv63jmg4Daum844","authorship_tag":"ABX9TyMmDO4P4rjPcYS/0W+5SDKI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#install the library to scrape pushshift\n","!pip install pmaw pandas"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eUKgmsXpJKe-","executionInfo":{"status":"ok","timestamp":1679532071928,"user_tz":240,"elapsed":13893,"user":{"displayName":"Luis Ruiz","userId":"02721672642318796029"}},"outputId":"785bc042-1a7c-4c3d-bce2-bb7a580d1890"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pmaw\n","  Downloading pmaw-3.0.0-py3-none-any.whl (29 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.4.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pmaw) (2.27.1)\n","Collecting praw\n","  Downloading praw-7.7.0-py3-none-any.whl (189 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.4/189.4 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2022.7.1)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from pandas) (1.22.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","Collecting prawcore<3,>=2.1\n","  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\n","Collecting update-checker>=0.18\n","  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n","Collecting websocket-client>=0.54.0\n","  Downloading websocket_client-1.5.1-py3-none-any.whl (55 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pmaw) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->pmaw) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->pmaw) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->pmaw) (2.0.12)\n","Installing collected packages: websocket-client, update-checker, prawcore, praw, pmaw\n","Successfully installed pmaw-3.0.0 praw-7.7.0 prawcore-2.3.0 update-checker-0.18.0 websocket-client-1.5.1\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from pmaw import PushshiftAPI\n","# import datetime as dt\n","from datetime import datetime, timedelta\n","import numpy as np\n","from itertools import product\n","import unicodedata as ud\n","\n","def format_df(df_original):\n","  # Keep only relevant columns\n","  df = df_original.copy()\n","  df = df[['subreddit','subreddit_id','author','author_fullname','author_flair_text','body', 'created_utc','id']]\n","  # Clean the flair text\n","  df['author_flair_text'] = df['author_flair_text'].apply(clean_flair)\n","  # Keep only rows with flair\n","  df = df[df['author_flair_text'].notna()]\n","  return df\n","\n","def clean_flair(flair):\n","  if type(flair) == str:\n","    mbti_types = [''.join(tup) for tup in list(product(['e', 'i'],['s', 'n'], ['f', 't'],['j', 'p']))]\n","    flair = ''.join(e for e in flair if e.isalnum()) #strip non-alphabetical numeric characters\n","    flair = ud.normalize('NFKD', flair) #normalize unicode representation\n","    flair = flair.lower() #make lowercase\n","    try:\n","      # Extract the mbti type from the flair string by:\n","      # (1) Find occurences of mbti types in flair and store the index of the first occurence\n","      # example: 'anesfpmarriedtointp' returns 2, the index of the character 'e'\n","      idx = min(i for i in [flair.find(x) for x in mbti_types] if i >= 0)\n","      #(2) make the flair be the first mbti occurence, example above returns 'esfp'\n","      flair = flair[idx:idx+4]\n","    except:\n","      # No match was found, so not useful\n","      flair = np.nan\n","  else:\n","    flair = np.nan\n","  return flair\n","\n","# Make a list w all subreddit name (ie mbti types)\n","subreddits = [''.join(tup) for tup in list(product(['e', 'i'],['s', 'n'], ['f', 't'],['j', 'p']))]\n","\n","# Define scrape window\n","start_date = datetime(2022, 11, 3)\n","end_date = datetime(2023, 3, 21)\n","\n","# Initialize pushshift api instance\n","api = PushshiftAPI()\n","\n","# Scrape\n","dfs = []  # to store the results of each api call\n","\n","for sub in subreddits:\n","  #Scrapping each sub in 1-month-long time windows\n","  current_date = start_date\n","  while current_date <= end_date:\n","    since = int(current_date.timestamp())  # start date\n","    until = int((current_date + timedelta(weeks=1)).timestamp())      # end date\n","    # Only scrapping 1000 comments per call because api breaks for >1000\n","    comments = api.search_comments(         \n","                        subreddit=sub,\n","                        since = since,\n","                        until = until,\n","                        limit=1000,\n","                        fields=[['author','author_fullname','subreddit','subreddit_id','id','body','author_flair_text', 'created_utc']],\n","                        safe_exit=True\n","                        )\n","    # Some days might have no user activity, so we need try/catch\n","    try:\n","      # Place results in dataframe\n","      df = pd.DataFrame(comments)\n","      # Format the results\n","      df = format_df(df)\n","      # Save to CSV file\n","      fname = '/content/drive/MyDrive/Classes/EECS_448/Project/mbti_scrape/{}_{}_{}_{}.csv'.format(sub, str(since), str(until), len(df.index))\n","      df.to_csv(fname)\n","      dfs.append(df)\n","    except:\n","      print('Exception raised. r/{}, search data: {}'.format(sub, since))\n","    \n","    # Increase the data tracker\n","    current_date += timedelta(weeks=1)\n","\n","all_df = pd.concat(dfs, ignore_index=True)"],"metadata":{"id":"1LfAHnHgJNcT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save dataframe of entire scrape to csv\n","fname = '/content/drive/MyDrive/Classes/EECS_448/Project/mbti_fullScrape.csv'\n","all_df.to_csv(fname)\n","\n","# make authors dataframe\n","authors_df = all_df[['author','author_flair_text']]\n","authors_df = authors_df.drop_duplicates(subset=['author'])\n","\n","# save authors dataframe to csv\n","fname = '/content/drive/MyDrive/Classes/EECS_448/Project/mbti_allLabledAuthors.csv'\n","authors_df.to_csv(fname)\n","authors_df.head()\n","\n","# user counts per type\n","counts = authors_df['author_flair_text'].value_counts()\n","print(counts)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ow_hTrejPN3","executionInfo":{"status":"ok","timestamp":1679543723821,"user_tz":240,"elapsed":260,"user":{"displayName":"Luis Ruiz","userId":"02721672642318796029"}},"outputId":"fd95633e-8dee-4fc8-dbbb-c30995e5eabf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["intp    1658\n","intj    1567\n","infp    1453\n","infj    1262\n","entp    1155\n","enfp    1007\n","istp     786\n","entj     592\n","isfp     340\n","enfj     324\n","istj     248\n","estp     201\n","isfj     172\n","esfp     117\n","estj      76\n","esfj      64\n","Name: author_flair_text, dtype: int64\n"]}]},{"cell_type":"markdown","source":["### Playground / Graveyard"],"metadata":{"id":"c_z4MjSUcVsv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"H2yq-HMMI1j8"},"outputs":[],"source":["import pandas as pd\n","from pmaw import PushshiftAPI\n","import datetime as dt\n","import numpy as np\n","from itertools import product\n","import unicodedata as ud\n","\n","def format_df(df_original):\n","  # Keep only relevant columns\n","  df = df_original.copy()\n","  df = df[['subreddit','subreddit_id','author','author_fullname','author_flair_text','body', 'created_utc','id']]\n","  # Clean the flair text\n","  df['author_flair_text'] = df['author_flair_text'].apply(clean_flair)\n","  # Keep only rows with flair\n","  df = df[df['author_flair_text'].notna()]\n","  return df\n","\n","def clean_flair(flair):\n","  if type(flair) == str:\n","    mbti_types = [''.join(tup) for tup in list(product(['e', 'i'],['s', 'n'], ['f', 't'],['j', 'p']))]\n","    flair = ''.join(e for e in flair if e.isalpha()) #strip non-alphabetical characters\n","    flair = ud.normalize('NFKD', flair) #normalize unicode representation\n","    flair = flair.lower() #make lowercase\n","    try:\n","      # Extract the mbti type from the flair string by:\n","      # (1) Find occurences of mbti types in flair and store the index of the first occurence\n","      # example: 'anesfpmarriedtointp' returns 2, the index of the character 'e'\n","      idx = min(i for i in [flair.find(x) for x in mbti_types] if i >= 0)\n","      #(2) make the flair be the first mbti occurence, example above returns 'esfp'\n","      flair = flair[idx:idx+4]\n","    except:\n","      # No match was found, so not useful\n","      flair = np.nan\n","  else:\n","    flair = np.nan\n","  return flair\n","\n","# Make a list w all subreddit name (ie mbti types)\n","subreddits = [''.join(tup) for tup in list(product(['e', 'i'],['s', 'n'], ['f', 't'],['j', 'p']))]\n","\n","# define lists with years and months used to define scrapping time window\n","years = [2022, 2022, 2023, 2023, 2023]\n","months = [11, 12, 1, 2, 3]\n","\n","# Initialize pushshift api instance\n","api = PushshiftAPI()\n","\n","# Scrape\n","dfs = []  # to store the results of each api call\n","\n","for sub in subreddits:\n","  #Scrapping each sub in 1-month-long time windows\n","  for i in range(len(years)-1):\n","    start_year, end_year, start_month, end_month = years[i], years[i+1], months[i], months[i+1]\n","    since = int(dt.datetime(start_year, start_month, 3,0,0).timestamp())  # start date\n","    until = int(dt.datetime(end_year, end_month, 2,0,0).timestamp())      # end date\n","    # Only scrapping 1000 comments per call because api breaks for >1000\n","    comments = api.search_comments(         \n","                        subreddit=sub,\n","                        since = since,\n","                        until = until,\n","                        limit=1000,\n","                        fields=[['author','author_fullname','subreddit','subreddit_id','id','body','author_flair_text', 'created_utc']],\n","                        safe_exit=True\n","                        )\n","    # Place results in dataframe\n","    df = pd.DataFrame(comments)\n","    # Format the results\n","    df = format_df(df)\n","    # Save to CSV file\n","    fname = '/content/drive/MyDrive/Classes/EECS_448/Project/scrapping_files/{}_{}_{}_{}.csv'.format(sub, str(since), str(until), len(df.index))\n","    df.to_csv(fname)\n","    dfs.append(df)\n","\n","all_df = pd.concat(dfs, ignore_index=True)"]},{"cell_type":"code","source":["# all_df.columns\n","# test = (all_df.author_flair_css_class.to_numpy())\n","\n","# np.unique(all_df.author_fullname.to_numpy()).shape\n","# all_df.subreddit.unique()\n","# all_df.apply(lambda x: x.astype(str).str.lower()).subreddit.unique()\n","# all_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PS5gYT4xURMb","executionInfo":{"status":"ok","timestamp":1679535474241,"user_tz":240,"elapsed":3,"user":{"displayName":"Luis Ruiz","userId":"02721672642318796029"}},"outputId":"d3721870-416b-49fb-a929-9099e616a218"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5398,)"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["authors_df = all_df[['author','author_flair_text']]\n","authors_df = authors_df.drop_duplicates()\n","authors_df.index"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dX9e2MnWGlZM","executionInfo":{"status":"ok","timestamp":1679535652050,"user_tz":240,"elapsed":136,"user":{"displayName":"Luis Ruiz","userId":"02721672642318796029"}},"outputId":"2f91dd4d-cbde-4da8-ec0e-bd9b67a61a92"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Int64Index([    0,     1,     2,     3,     4,     5,     6,     9,    12,\n","               16,\n","            ...\n","            28216, 28218, 28220, 28224, 28228, 28229, 28231, 28232, 28236,\n","            28239],\n","           dtype='int64', length=5504)"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["# api = PushshiftAPI()\n","# dfs = []\n","# for sub in subreddits:\n","#   for i in range(len(years)-1):\n","#     start_year, end_year, start_month, end_month = years[i], years[i+1], months[i], months[i+1]\n","#     since = int(dt.datetime(start_year, start_month, 3,0,0).timestamp())\n","#     until = int(dt.datetime(end_year, end_month, 2,0,0).timestamp())\n","#     comments = api.search_comments(         \n","#                         subreddit=sub,\n","#                         since = since,\n","#                         until = until,\n","#                         limit=1000,\n","#                         safe_exit=True\n","#                         )\n","#     df = pd.DataFrame(comments)\n","#     fname = '/content/drive/MyDrive/Classes/EECS_448/Project/scrapping_files/{}_{}_{}_{}.csv'.format(sub, str(since), str(until), len(comments))\n","#     df.to_csv(fname)\n","#     dfs.append(df)\n"],"metadata":{"id":"GiCt8_N4Jt1r"},"execution_count":null,"outputs":[]}]}